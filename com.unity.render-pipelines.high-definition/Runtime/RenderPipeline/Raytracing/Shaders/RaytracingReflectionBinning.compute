#pragma kernel ClearBinCounts
#pragma kernel Prepass
//#pragma kernel CountsToOffsets
//#pragma kernel CalculateBins

#pragma enable_d3d11_debug_symbols

//#pragma only_renderers d3d11
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/ShaderLibrary/ShaderVariables.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/Material/NormalBuffer.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Color.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/Material/Builtin/BuiltinData.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/BSDF.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/CommonLighting.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/ImageBasedLighting.hlsl"
//#include "Packages/com.unity.render-pipelines.high-definition/Runtime/Material/PreIntegratedFGD/PreIntegratedFGD.hlsl"
//#include "Packages/com.unity.render-pipelines.high-definition/Runtime/PostProcessing/Shaders/TemporalAntialiasing.hlsl"

// Raytracing Includes
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/OnlineVariance.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/RaytracingConsts.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/RaytracingSampling.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/ShaderVariablesRaytracing.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/Lighting/ScreenSpaceLighting/ScreenSpaceLighting.hlsl"

RWTexture2D<uint>                  _RTReflBinCounts;
RWTexture2D<uint>                  _RTReflBinOffsets;
RWTexture2D<uint>                  _RTReflBinTemp;
Texture2D<float>                   _DepthTexture;
Texture2D<float4>                  _SsrClearCoatMaskTexture;

uint _RTReflBinMax;
uint _RTReflWidth;
uint _RTReflHeight;  

[numthreads(64, 1, 1)]
void ClearBinCounts(uint3 dispatchThreadId : SV_DispatchThreadID, uint2 groupThreadId : SV_GroupThreadID, uint2 groupId : SV_GroupID)
{
    if (dispatchThreadId.x < _RTReflBinMax && dispatchThreadId.y<1)
    {
        _RTReflBinCounts[dispatchThreadId.xy] = 0; 
    }
}

[numthreads(8, 8, 1)]
void Prepass(uint3 dispatchThreadId : SV_DispatchThreadID, uint2 groupThreadId : SV_GroupThreadID, uint2 groupId : SV_GroupID)
{
#if 1

    if (dispatchThreadId.x < _RTReflWidth && dispatchThreadId.y < _RTReflHeight)
    {
        uint2 LaunchDim = uint2(_RTReflWidth, _RTReflHeight);
        // Compute the pixel coordinate to evaluate
        uint2 currentCoord = uint2(dispatchThreadId.x, _RTReflHeight - dispatchThreadId.y - 1);

        // Get the scramblingValue of this pixel
        uint2 scramblingValue = ScramblingValue(currentCoord.x, currentCoord.y);

        // Read the depth value
        float depthValue = _DepthTexture[currentCoord]; 

        // This point is part of the background, we don't really care
        if (depthValue == UNITY_RAW_FAR_CLIP_VALUE)
        {
            _RTReflBinTemp[dispatchThreadId.xy].x = 0xffffffff;
            return;
        }
        PositionInputs posInput = GetPositionInput(currentCoord, 1.0f / LaunchDim.xy, depthValue, _InvViewProjMatrix, _ViewMatrix, 0);

        float3 positionWS = GetAbsolutePositionWS(posInput.positionWS);

        // Compute the incident vector on the surfaces
        float3 viewWS = normalize(_WorldSpaceCameraPos - positionWS);

        // Decode the world space normal
        NormalData normalData;
        DecodeFromNormalBuffer(currentCoord, normalData);
        // Override the roughness by the clearcoat value of this is a clear coat
        float4 coatMask = _SsrClearCoatMaskTexture[currentCoord];
        normalData.perceptualRoughness = HasClearCoatMask(coatMask) ? CLEAR_COAT_PERCEPTUAL_ROUGHNESS : normalData.perceptualRoughness;

        // Create the local ortho basis
        float3x3 localToWorld = GetLocalFrame(normalData.normalWS);

        // If this value is beyond the smothness that we allow, no need to compute it
        if (_RaytracingReflectionMinSmoothness > PerceptualRoughnessToPerceptualSmoothness(normalData.perceptualRoughness))
        {
            _RTReflBinTemp[dispatchThreadId.xy].x = 0xffffffff;
            return;
        }

        // Compute the actual roughness
        float roughness = PerceptualRoughnessToRoughness(normalData.perceptualRoughness);

        // Clamp the sample count if the surface is a completely smooth
        int realSampleCount = normalData.perceptualRoughness < 0.01 ? 1 : _RaytracingNumSamples;

        // Compute the current sample index
        int globalSampleIndex = _RaytracingFrameIndex * realSampleCount;

        // Generate the new sample (follwing values of the sequence)
        float2 sample = float2(0.0, 0.0);
        sample.x = GetRaytracingNoiseSample(globalSampleIndex, 0, scramblingValue.x);
        sample.y = GetRaytracingNoiseSample(globalSampleIndex, 1, scramblingValue.y);

        // Importance sample the direction using GGX
        float3 sampleDir = float3(0.0, 0.0, 0.0);
        float NdotL, NdotH, VdotH, LdotH;
        SampleGGXDir2(sample, viewWS, localToWorld, roughness, sampleDir, NdotL, NdotH, VdotH, LdotH, false);

        float zen = acos(sampleDir.z);
        float az = atan2(sampleDir.y, sampleDir.x);

        float pi = 3.141592654;
        zen = zen / pi * 8;
        az = (az + pi) / (2.0 * pi) * 16.0;

        uint2 tile = dispatchThreadId.xy / (LaunchDim.xy / uint2(4, 4));
        tile = clamp(tile, 0, 3);

        int bin = (tile.x) | (tile.y << 2) | (uint(zen) << 4) | (uint(az) << 7);

        uint2 idx = uint2(bin, 0);
        uint curoffset;
        InterlockedAdd(_RTReflBinCounts[idx], 1, curoffset);		//atomic add to bin counts and return current offset into bin

        uint packedint = (curoffset << 11) | bin;       			//assumes a lot
        _RTReflBinTemp[dispatchThreadId.xy].x = packedint;

    }
#endif
}

//Sort each group bins (2048 / 256 so 8 bins) NO, cos 1 thread does two entries
//Sort _RTReflBinCounts into _RTReflBinOffsets

//TODO. Find out how to make these 1d buffers that also work in Raytracing/Raygen shaders
RWTexture2D<uint>   _ScanCounts;
RWTexture2D<uint>   _ScanOffsets;
RWTexture2D<uint>   _ScanBlocksums;
int                 _ScanStoreBlocksums;
int                 _ScanMax;
//int                 _ScanThreadGroupSize;   //128, processing 256 items per thread group

groupshared int scanSharedMem[256];	
[numthreads(128, 1, 1)]
void Scan(uint3 id : SV_DispatchThreadID)
{
    //TODO, avoid bank conflicts?
    int thid = id.x&127;    //id actually 0-127
    int rwid = (id.x * 2);

    int offset = 1;

    scanSharedMem[2 * thid] = _ScanCounts[uint2(rwid, 0)].x;
    scanSharedMem[2 * thid + 1] = _ScanCounts[uint2(rwid + 1, 0)].x;

    int finalCount = 0;
    if (thid == 127)
    {
        finalCount = scanSharedMem[255];
    }

    int d;
    for (d = (int)256 >> 1; d > 0; d >>= 1)
    {
        GroupMemoryBarrierWithGroupSync();
        if (thid < d)
        {
            int ai = offset * (2 * thid + 1) - 1;
            int bi = offset * (2 * thid + 2) - 1;
            scanSharedMem[bi] += scanSharedMem[ai];
        }
        offset *= 2;
    }

    if (thid == 0)
        scanSharedMem[255] = 0;

    for (d = 1; d < (int)256; d *= 2)
    {
        offset >>= 1;
        GroupMemoryBarrierWithGroupSync();
        if (thid < d)
        {
            int ai = offset * (2 * thid + 1) - 1;
            int bi = offset * (2 * thid + 2) - 1;
            int t = scanSharedMem[ai];
            scanSharedMem[ai] = scanSharedMem[bi];
            scanSharedMem[bi] += t;
        }
    }
    GroupMemoryBarrierWithGroupSync();

    _ScanOffsets[uint2(rwid, 0)].x = scanSharedMem[2 * thid];
    _ScanOffsets[uint2(rwid + 1, 0)].x = scanSharedMem[2 * thid + 1];

    if (thid == 127)
    {
        if (_ScanStoreBlocksums)
        {
            _ScanBlocksums[id.x / 128].x = finalCount + scanSharedMem[255];
        }
    }
}

//find parallel prefix sum for initial valus in _RTReflBinOffsets[0],[256],[512],[768],[1024] etc
//store into _RTReflBinCounts[0-7]
[numthreads(128, 1, 1)]
void AddBlockSumsToOffsets(uint3 id : SV_DispatchThreadID)
{

}


[numthreads(8, 8, 1)]
void CalculateBins(uint3 dispatchThreadId : SV_DispatchThreadID, uint2 groupThreadId : SV_GroupThreadID, uint2 groupId : SV_GroupID)
{
}


//============================================================================
#if 0
// Input textures prepass
Texture2D<float>                    _DepthTexture;
Texture2DArray<float>               _NoiseTexture;
Texture2D<float4>                   _SsrLightingTextureRW;
Texture2D<float4>                   _SsrHitPointTexture;
Texture2D<float4>                   _SsrClearCoatMaskTexture;

// Output Textures for the spatial filtering
RWTexture2D<float4>                 _RaytracingReflectionTexture;
RWTexture2D<float>                  _VarianceTexture;
RWTexture2D<float3>                 _MaxColorRangeTexture;
RWTexture2D<float3>                 _MinColorRangeTexture;
int                                 _SpatialFilterRadius;

// Input and Output data of the temporal accumulation pass
RWTexture2D<float4>                 _CurrentFrameTexture;
RWTexture2D<float4>                 _AccumulatedFrameTexture;
float                               _TemporalAccumuationWeight;

// Buffers for the temporal filtering
Texture2D<float4>           _ReflectionHistorybufferRW;
Texture2D<float4>           _DenoiseInputTexture;
RWTexture2D<float4>         _DenoiseOutputTextureRW;
int                         _RaytracingDenoiseRadius;

[numthreads(RAYTRACING_REFLECTION_TILE_SIZE, RAYTRACING_REFLECTION_TILE_SIZE, 1)]
void RaytracingReflectionFilter(uint3 dispatchThreadId : SV_DispatchThreadID, uint2 groupThreadId : SV_GroupThreadID, uint2 groupId : SV_GroupID)
{

    uint2 fullResCoord = dispatchThreadId.xy;
    uint2 halfResCoord = fullResCoord / 2;

    // Compute the index of the noise texture to use
    int noiseIndex = (int)(clamp((int)(_ScramblingTexture[halfResCoord].y * 32.0f), 0, 31));

    // Compute the subpixel index that matches this full screen pixel.
    int localIndex = (fullResCoord.x & 1) + (fullResCoord.y & 1) * 2;

    // Fetch the depth
    float depth = LOAD_TEXTURE2D(_DepthTexture, fullResCoord).x;

    NormalData normalData;
    DecodeFromNormalBuffer(fullResCoord, normalData);

    // We use a texture to identify if we use a clear coat constant for perceptualRoughness for SSR or use value from normal buffer.
    // When we use a forward material we can output the normal and perceptualRoughness for the coat for SSR, so we simply bind a black 1x1 texture
    // When we use deferred material we need to bind the gbuffer2 and read the coat mask
    float4 coatMask = _SsrClearCoatMaskTexture[fullResCoord];
    normalData.perceptualRoughness = HasClearCoatMask(coatMask) ? CLEAR_COAT_PERCEPTUAL_ROUGHNESS : normalData.perceptualRoughness;
    // Fetch the roughness
    float roughness = PerceptualRoughnessToRoughness(normalData.perceptualRoughness);

    // Duplicating same early out condition we do on reflection dispatchrays as that info is 1/2 res while we need full res granularity here.
    // Also, this operates on data we fetch anyway, while the _SsrLightingTextureRW at central pixel is needed only if that pixel contributes to filtering below. 
    if (depth == UNITY_RAW_FAR_CLIP_VALUE || PerceptualRoughnessToPerceptualSmoothness(normalData.perceptualRoughness) < _RaytracingReflectionMinSmoothness)
        return;

    // Fetch the normal WS
    float3 normalWS = normalData.normalWS;

    // Compute the world space position
    PositionInputs posInput = GetPositionInput_Stereo(fullResCoord, _ScreenSize.zw, depth, UNITY_MATRIX_I_VP, UNITY_MATRIX_V, unity_StereoEyeIndex);
    float3 positionWS = GetAbsolutePositionWS(posInput.positionWS);

    // Compute the view in world space
    float3 viewWS = normalize(_WorldSpaceCameraPos - positionWS);

    // Compute the reflected direction for this view direction
    float3 reflDir = reflect(-viewWS, normalWS);

    // Initialize the output pixels
    float4 resultSum = float4(0.0 ,0.0, 0.0, 0.0);
    float3 minColorRange = float3(1000.0, 1000.0, 1000.0);
    float3 maxColorRange = float3(0.0, 0.0, 0.0);
    uint sampleCount = 0;

    VarianceEstimator variance;
    InitializeVarianceEstimator(variance);

    float radiusSq = _SpatialFilterRadius * _SpatialFilterRadius;
    
    for(int y = -_SpatialFilterRadius; y < _SpatialFilterRadius; ++y)
    {
        for(int x = -_SpatialFilterRadius; x < _SpatialFilterRadius; ++x)
        {
            float radiusDistanceSq = (y*y + x*x);
            if(radiusDistanceSq > radiusSq) continue;

            // Compute the noise position that shall be used
            int2 relativeHRShift = uint2(8 + x, 8 + y);

            // Full res sample position
            int2 sourceCoord = (halfResCoord + uint2(x,y)) * 2;

            // If this pixel is outside of the screen, we cannot use it
            if(sourceCoord.x < 0 || sourceCoord.x > _ScreenSize.x 
                || sourceCoord.y < 0 || sourceCoord.y > _ScreenSize.y) 
            continue;
            
            // Fetch the target color
            float4 sampleColor = _SsrLightingTextureRW[sourceCoord];

            // Compute the position of the actual source pixel
            uint subPixel =  clamp(floor(sampleColor.w * 4.0f), 0, 3);
            uint2 shift = HalfResIndexToCoordinateShift[subPixel];
            uint2 actualSourceCoord = sourceCoord + shift;

            // Fetch the Depth
            float sampleDepth = LOAD_TEXTURE2D(_DepthTexture, actualSourceCoord).x;
            // If this the background, it should not be used as a valid sample
            if(sampleDepth == 0.0f) continue;

            // Compute the target pixel that it will impact
            float sample = _NoiseTexture[int3(relativeHRShift, noiseIndex)].x;
            int index = clamp(floor(sample * 4.0f), 0, 3);

            if (index != localIndex) continue;

            // Let's fetch the half res sample's properties
            // Get the direction and pdf
            float4 directionPDF = _SsrHitPointTexture[sourceCoord];

            // If this direction is under the candidate surface, then it is not valid
            if(dot(directionPDF.xyz, normalWS) <= 0.0f) continue;

            // If this direction is not in the hemisphere of the reflected view direction, then it is not valid
            if(dot(directionPDF.xyz, reflDir) <= 0.0f) continue;

            // Compute the brdf of this sample
            float weight = 1.0f;
            if(roughness > 0.001)
            {
                // Compute the brdf of this sample
                float3 H = normalize(directionPDF.xyz + viewWS);
                float NdotH = dot(normalWS, H);
                float NdotL = dot(directionPDF.xyz, normalWS);
                float NdotV = dot(viewWS, normalWS);
                float localBRDF = D_GGX(NdotH, roughness) * V_SmithJointGGX(NdotL, NdotV, roughness) * NdotL;
                weight = localBRDF * directionPDF.w;
            }

            // Push the value to the variance estimation
            PushValue(variance, length(sampleColor.xyz));

            // Contirbute to all the output values
            float3 sampleResult = sampleColor.xyz * weight;
            resultSum += float4(sampleResult, weight);
            minColorRange = min(minColorRange[index], sampleResult);
            maxColorRange = max(maxColorRange[index], sampleResult);
            sampleCount += 1;
        }
    }

        // Compute the full res coordinate
        if(depth == 0.0f || sampleCount == 0)
        {
            _RaytracingReflectionTexture[fullResCoord] = float4(0.0f, 0.0f, 0.0f, 0.0f);
            _VarianceTexture[fullResCoord] = 1.0f;
            _MaxColorRangeTexture[fullResCoord] = float3(0.0, 0.0, 0.0);
            _MinColorRangeTexture[fullResCoord] = float3(1.0, 1.0, 1.0);
        }
        else
        {
            _RaytracingReflectionTexture[fullResCoord] = float4((resultSum.xyz / resultSum.w), roughness);
            _VarianceTexture[fullResCoord] = saturate(Variance(variance));
            _MaxColorRangeTexture[fullResCoord] = maxColorRange;
            _MinColorRangeTexture[fullResCoord] = minColorRange;
        }
}

[numthreads(RAYTRACING_REFLECTION_TILE_SIZE, RAYTRACING_REFLECTION_TILE_SIZE, 1)]
void TemporalAccumulationFilter(uint2 groupThreadId : SV_GroupThreadID, uint2 groupId : SV_GroupID)
{
    // Fetch the current pixel coordinate
    uint2 currentCoord = groupId * RAYTRACING_REFLECTION_TILE_SIZE + groupThreadId;
    currentCoord.x = currentCoord.x + (unity_StereoEyeIndex * _ScreenSize.x);

    // Fetch the previous color
    float3 previousColor = _AccumulatedFrameTexture[currentCoord].xyz;
    bool previousValidityFlag = _AccumulatedFrameTexture[currentCoord].w > 0.0f;

    // Fetch the color range that we need to check before using
    float3 colorMinBound = _MinColorRangeTexture[currentCoord].xyz;
    float3 colorMaxBound = _MaxColorRangeTexture[currentCoord].xyz;

    // check if the previous color is in the bounds
    // TODO: Try to do the comparison in Lab for better results http://www.brucelindbloom.com/index.html?Math.html
    bool colorInBound = colorMinBound.x < previousColor.x && colorMaxBound.x > previousColor.x 
                        && colorMinBound.y < previousColor.y && colorMaxBound.y > previousColor.y 
                        && colorMinBound.z < previousColor.z && colorMaxBound.z > previousColor.z;

    // Validity flag of the current sample
    float validityFlag = all(colorMinBound > colorMaxBound);
    
    float3 combinedColor = float3(0.0f, 0.0f, 0.0f);
    if(previousValidityFlag && colorInBound)
    {
        // Compute the accumulation factor for this surface (using the user parameter and the rouhgness of the surface)
        float accumulationFactor = _CurrentFrameTexture[currentCoord].w < 0.001f ? 1.0 : _TemporalAccumuationWeight;  

        // Previous pixel is valid
        combinedColor = (_CurrentFrameTexture[currentCoord].xyz * accumulationFactor + _AccumulatedFrameTexture[currentCoord].xyz * (1.0 - accumulationFactor));
    }
    else
    {
        // Previous pixel is invalid, override it
        combinedColor = _CurrentFrameTexture[currentCoord].xyz;
    }
    
    _AccumulatedFrameTexture[currentCoord] = float4(combinedColor, validityFlag);
    _CurrentFrameTexture[currentCoord] = float4(combinedColor, validityFlag);
}

[numthreads(RAYTRACING_REFLECTION_TILE_SIZE, RAYTRACING_REFLECTION_TILE_SIZE, 1)]
void RaytracingReflectionTAA(uint2 groupThreadId : SV_GroupThreadID, uint2 groupId : SV_GroupID)
{
    // Fetch the current pixel coordinate
    uint2 centerCoord = groupId * RAYTRACING_REFLECTION_TILE_SIZE + groupThreadId;
    centerCoord.x = centerCoord.x + (unity_StereoEyeIndex * _ScreenSize.x);

    float depth = LOAD_TEXTURE2D(_DepthTexture, centerCoord).r;
    PositionInputs posInputs = GetPositionInput_Stereo(centerCoord, _ScreenSize.zw, depth, UNITY_MATRIX_I_VP, UNITY_MATRIX_V, unity_StereoEyeIndex);

    float2 closest = GetClosestFragment(posInputs);

    float2 velocity;
    DecodeVelocity(LOAD_TEXTURE2D_X(_CameraMotionVectorsTexture, closest), velocity);
    float velocityLength = length(velocity);

    float2 uv = posInputs.positionNDC;

    float3 color = Fetch(_DenoiseInputTexture, uv, 0.0, _ScreenToTargetScale.xy);
    float3 history = Fetch(_ReflectionHistorybufferRW, posInputs.positionNDC - velocity, 0.0, _ScreenToTargetScaleHistory.xy);
    float3 topLeft = Fetch(_DenoiseInputTexture, uv, -RADIUS, _ScreenToTargetScale.xy);
    float3 bottomRight = Fetch(_DenoiseInputTexture, uv, RADIUS, _ScreenToTargetScale.xy);

    float3 corners = 4.0 * (topLeft + bottomRight) - 2.0 * color;

    color = clamp(color, 0.0, CLAMP_MAX);

    float3 average = Map((corners + color) / 7.0);

    topLeft = Map(topLeft);
    bottomRight = Map(bottomRight);
    color = Map(color);

    float colorLuma = Luminance(color);
    float averageLuma = Luminance(average);
    float nudge = lerp(4.0, 0.25, saturate(velocityLength * 100.0)) * abs(averageLuma - colorLuma);

    float3 minimum = min(bottomRight, topLeft) - nudge;
    float3 maximum = max(topLeft, bottomRight) + nudge;

    history = Map(history);

    // Clip history samples
    history = ClipToAABB(history, minimum, maximum);

    // Blend color & history
    // Feedback weight from unbiased luminance diff (Timothy Lottes)
    float historyLuma = Luminance(history);
    float diff = abs(colorLuma - historyLuma) / Max3(colorLuma, historyLuma, 0.2);
    float weight = 1.0 - diff;
    float feedback = lerp(FEEDBACK_MIN, FEEDBACK_MAX, weight * weight);

    color = Unmap(lerp(color, history, feedback));
    color = clamp(color, 0.0, CLAMP_MAX);
    
    _DenoiseOutputTextureRW[centerCoord] = float4(color, _DenoiseInputTexture[centerCoord].w);
}

// ----------------------------------------------------------------------------
// Denoising Kernel
// ----------------------------------------------------------------------------

// Couple helper functions
float sqr(float value)
{
    return value * value;
}
float gaussian(float radius, float sigma)
{
    return exp(-sqr(radius / sigma));
}

// Bilateral filter parameters
#define NORMAL_WEIGHT   1.0
#define PLANE_WEIGHT    1.0
#define DEPTH_WEIGHT    1.0

struct BilateralData
{
    float3 position;
    float3 normal;
    float perceptualRoughness;
    float  z;
};

BilateralData TapBilateralData(uint2 coordSS)
{
    BilateralData key;
    PositionInputs posInput;

    if (DEPTH_WEIGHT > 0.0 || PLANE_WEIGHT > 0.0)
    {
        posInput.deviceDepth = LOAD_TEXTURE2D(_DepthTexture, coordSS).r;
        key.z = Linear01Depth(posInput.deviceDepth, _ZBufferParams);
    }

    if (PLANE_WEIGHT > 0.0)
    {
        posInput = GetPositionInput_Stereo(coordSS, _ScreenSize.zw, posInput.deviceDepth,
                                           UNITY_MATRIX_I_VP, UNITY_MATRIX_V, unity_StereoEyeIndex);
        key.position = posInput.positionWS;
    }

    if ((NORMAL_WEIGHT > 0.0) || (PLANE_WEIGHT > 0.0))
    {
        NormalData normalData;
        const float4 normalBuffer = _NormalBufferTexture[COORD_TEXTURE2D_X(coordSS)];
        DecodeFromNormalBuffer(normalBuffer, coordSS, normalData);
        key.normal = normalData.normalWS;
        key.perceptualRoughness = normalData.perceptualRoughness;
    }

    return key;
}

float ComputeBilateralWeight(BilateralData center, BilateralData tap)
{
    float depthWeight    = 1.0;
    float normalWeight   = 1.0;
    float planeWeight    = 1.0;

    if (DEPTH_WEIGHT > 0.0)
    {
        depthWeight = max(0.0, 1.0 - abs(tap.z - center.z) * DEPTH_WEIGHT);
    }

    if (NORMAL_WEIGHT > 0.0)
    {
        const float normalCloseness = sqr(sqr(max(0.0, dot(tap.normal, center.normal))));
        const float normalError = 1.0 - normalCloseness;
        normalWeight = max(0.0, (1.0 - normalError * NORMAL_WEIGHT));
    }

    if (PLANE_WEIGHT > 0.0)
    {
        // Change in position in camera space
        const float3 dq = center.position - tap.position;

        // How far away is this point from the original sample
        // in camera space? (Max value is unbounded)
        const float distance2 = dot(dq, dq);

        // How far off the expected plane (on the perpendicular) is this point? Max value is unbounded.
        const float planeError = max(abs(dot(dq, tap.normal)), abs(dot(dq, center.normal)));

        planeWeight = (distance2 < 0.0001) ? 1.0 :
            pow(max(0.0, 1.0 - 2.0 * PLANE_WEIGHT * planeError / sqrt(distance2)), 2.0);
    }

    return depthWeight * normalWeight * planeWeight;
}

// Separated bilateral filter (two passes, each with 2*Radius taps)
[numthreads(RAYTRACING_REFLECTION_TILE_SIZE, RAYTRACING_REFLECTION_TILE_SIZE, 1)]
void ReflBilateralFilter(uint2 groupThreadId : SV_GroupThreadID, uint2 groupId : SV_GroupID)
{
    // Fetch the current pixel coordinate
    uint2 centerCoord = groupId * RAYTRACING_REFLECTION_TILE_SIZE + groupThreadId;
    centerCoord.x = centerCoord.x + (unity_StereoEyeIndex * _ScreenSize.x);

    float3 reflSum = 0.0;
    float wSum = 0.0;

    #if FINAL_PASS
    const uint2 passIncr = uint2(1, 0);
    #else
    const uint2 passIncr = uint2(0, 1);
    #endif

    // Read the data of the center pixel
    const BilateralData center = TapBilateralData(centerCoord);

    // In order to avoid over-blurring, we define the size of the kernel based on the roughness of the surface
    const float realRadius = max(1, _RaytracingDenoiseRadius * sqrt(center.perceptualRoughness));
    const float sigma = 0.5 * realRadius;
    const int effectiveRadius = min(sigma * 2.0, realRadius);

    uint2 tapCoord = centerCoord - effectiveRadius * passIncr;
    for (int r = -effectiveRadius; r <= effectiveRadius; ++r, tapCoord += passIncr)
    {
        // Compute the weight (skip computation for the center)
        const float w = r ? gaussian(r, sigma) * ComputeBilateralWeight(center, TapBilateralData(tapCoord)) : 1.0;

        reflSum += _DenoiseInputTexture[tapCoord].xyz * w;
        wSum += w;
    }

    // Store the intermediate result
    float3 reflection = reflSum / wSum;
    _DenoiseOutputTextureRW[centerCoord] = float4(reflection, 1.0);
}

#endif
